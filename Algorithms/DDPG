'''
The basis of this code can be found in https://github.com/NovemberChopin/RL_Tutorial/blob/master/code/DDPG.py 
Some changes and corrections have been made according my unsderstanding.
'''

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import tensorflow.keras.layers as kl
import tensorflow.keras.losses as kls
import tensorflow.keras.optimizers as ko
import gym
import logging
from gym import wrappers
import random



class ReplayBuffer:
    def __init__(self, buffer_size):
        self.buffer_size = buffer_size
        self.buffer = []
        self.pointer = 0

    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.buffer_size:
            self.buffer.append(None)
        self.buffer[self.pointer] = (state, action, reward, next_state, done)
        self.pointer = int((self.pointer+1) % self.buffer_size)

    def random_sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        batch_state, batch_action, batch_reward, batch_next_state, batch_done = map(np.stack, zip(*batch))

class Actor(tf.keras.Model):
    def __init__(self, act_dim, is_continuious, act_hidden_size, act_hidden_activation, act_output_activation, **kwargs):
        super().__init__('Actor_net') # name is A2C_net
        self.act_dim = act_dim
        self.is_continuious = is_continuious # This equals to 0, if actions are descrete!!! In this file, only descrete actions are currently support.
        # the layers of the network model is defined in init function
        self.act_hidden_layer = kl.Dense(act_hidden_size, activation = act_hidden_activation)
        self.act_output_layer = kl.Dense(act_dim, activation = act_output_activation) # shall be tanh if is_continuious
        self.action_range = kwargs.get('action_range') # shall be included if is_continuious == 1
        self.scale_layer = kl.Lambda(lambda x: x * kwargs.get('action_range'))
        self.var = kwargs.get('var')
        
    def call(self, inputs): 
    # call function is for defining forward propagation, inputs may be a mini-batch of states/observations, return the outputs
    # actor network represents the policy, the output from actor network is logits if actions are descrete
        x = tf.convert_to_tensor(inputs, dtype=tf.float32)
        x = self.act_hidden_layer(x)
        x = self.act_output_layer(x)
        if self.is_continuious:
            self.policy = self.scale_layer(x)
        else:
            self.policy = x

        return self.policy
  
    def get_nextAction(self, obs, **kwargs): #obs is a batch (batch_size=1) of samples
        policy = self.predict(obs)
        if self.is_continuious:
            action = np.clip(np.random.normal(policy, self.var), -self.action_range, self.action_range).astype(np.float32)
            return np.squeeze(action, axis=0) # shapes are (act_dim,) 
        else:
            action = tf.squeeze(tf.random.categorical(policy, 1), axis=-1) #the shape is (batch_size,)
            return np.squeeze(action, axis=-1) # shapes are (,) 


    
    
class Critic(tf.keras.Model):
    def __init__(self, act_dim, q_hidden_size, q_hidden_activation, q_output_activation, **kwargs):
        super().__init__('A2C_net') # name is A2C_net
        self.act_dim = act_dim
        # the layers of the network model is defined in init function
        self.q_hidden_layer = kl.Dense(q_hidden_size, activation = q_hidden_activation)
        self.q_output_layer = kl.Dense(1, activation = q_output_activation) 
        
    def call(self, inputs): 
    # call function is for defining forward propagation, inputs may be a mini-batch of states/observations, return the outputs
    # critic network reprents the q(s,a) value, the output from critic network is the value of q(s,a)
        y = tf.convert_to_tensor(inputs, dtype=tf.float32)
        y = self.q_hidden_layer(y)
        self.q = self.q_output_layer(y)
        return self.q
  
    def get_q(self, obs, **kwargs): #obs is a batch (batch_size=1) of samples
        q = self.predict(obs) # shape is (batch_size,1)
        return np.squeeze(q, axis=0) # shape is (1,) 
  
    
    
def copy_para(from_model, to_model):
    for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):
        j.assign(i)
    
class DDPGAgent:
    def __init__(self, actor, target_actor, critic, target_critic, replay_buffer):
        self.params = {'value': 0.5, 'entropy': 0.0001, 'gamma': 0.99, 'TAU': 0.01}
        self.actor = actor
        self.target_actor = target_actor
        self.critic = critic
        self.target_critic = target_critic
        copy_para(self.actor, self.target_actor)
        copy_para(self.critic, self.target_critic)
        
        self.optimizer=ko.RMSprop(lr=0.007)
        self.ema = tf.train.ExponentialMovingAverage(decay=1 - self.params['TAU'])
        self.replay_buffer = replay_buffer
    
    def ema_update(self):
        paras = self.actor.trainable_weights + self.critic.trainable_weights
        self.ema.apply(paras)
        for i, j in zip(self.target_actor.trainable_weights + self.target_critic.trainable_weights, paras):
            i.assign(self.ema.average(j))
    
    def learn(self, batch_sz=32):
        states, actions, rewards, states_, done = self.replay_buffer.sample(batch_sz)
        rewards = rewards[:, np.newaxis]
        done = done[:, np.newaxis]
        
        with tf.GradientTape() as tape:
            actions_= self.target_actor(states_)
            q_ = self.target_critic([states_, actions_])
            target = rewards + (1 - done) * self.params['gamma'] * q_
            q_pred = self.critic([states, actions])
            td_error = tf.losses.mean_squared_error(target, q_pred)
        critic_grads = tape.gradient(td_error, self.critic.trainable_weights)
        self.critic_opt.apply_gradients(zip(critic_grads, self.critic.trainable_weights))

        with tf.GradientTape() as tape:
            actions = self.actor(states)
            q = self.critic([states, actions])
            actor_loss = -tf.reduce_mean(q)  # maximize the q
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_weights)
        self.actor_opt.apply_gradients(zip(actor_grads, self.actor.trainable_weights))
        
        self.ema_update()
        
 
 
    def train(self, env, batch_sz=32, episodes_num=1000, maxStepsPerEpisode = 200):
        ep_rews = []
        for episode in range(episodes_num):
            ep_rews.append(0.0)
            next_obs = env.reset()
            state = next_obs.copy()
            for step in range(maxStepsPerEpisode):
                action = self.model.actor.get_nextAction
                next_obs, reward, done, info = env.step(action)
                state_=next_obs.copy()
                done = 1 if done is True else 0
                buffer.push(state, action, reward, state_, done)
                if len(buffer) >= self.replay_buffer.buffer_size:
                    self.learn()
                
                state = state_
                ep_rews[-1] += reward
                if done:
                    break
        return ep_rews


    def test(self, env, render=True):
        outdir = '/tmp/A2C-Cartpole-agent-results'
        env = wrappers.Monitor(env, directory=outdir, force=True) # This is for showing the result as a video.
        obs, done, ep_reward = env.reset(), False, 0
        while not done:
            action = self.actor.get_nextAction(obs[None, :])
            obs, reward, done, _ = env.step(action)
            ep_reward += reward
        return ep_reward

    
    
if __name__ == '__main__':
    logging.getLogger().setLevel(logging.DEBUG)


    env =gym.make('Pendulum-v0')

    if isinstance(env.action_space, gym.spaces.Box):
        is_continuious = True
        act_dim = env.action_space.shape[0]
        VAR = 2
        RANGE = env.action_space.high
        actor = Actor(act_dim, is_continuious, 128, 'relu', 'tanh', var=VAR, action_range=RANGE)
        critic = Critic(act_dim, 128, 'relu', None)
        target_actor = Actor(act_dim, is_continuious, 128, 'relu', 'tanh', var=VAR, action_range=RANGE)
        target_critic = Critic(act_dim, 128, 'relu', None)
        
    obs = env.reset()
    state = obs.copy()
    action = actor.get_nextAction(state[None, :])
    q = critic.get_q([state, action][None, :])
    target_action = actor.get_nextAction(state[None, :])
    target_q = critic.get_q([state, action][None, :])
    print(action, q, target_action, target_q) # 
    
    MEMORY_CAPACITY = 10000
    replay_buffer = ReplayBuffer(MEMORY_CAPACITY)
    agent = DDPGAgent(actor, target_actor, critic, target_critic, replay_buffer)
    
    rewards_sum = agent.test(env)
    print("%d out of 200" % rewards_sum) # The score in one test with random agent.
    rewards_history = agent.train(env)
    print("Finished training, testing...")
    print("%d out of 200" % agent.test(env)) # The score in one test with trained agent.
    
    #Plot the rewards of history episodes during tranining
    plt.plot(np.arange(0,len(rewards_history),1), rewards_history[::1])
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.show
